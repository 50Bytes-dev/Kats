# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

# pyre-unsafe

from typing import List, Optional, Tuple, Dict, Any
from unittest import TestCase

import numpy as np
import pandas as pd
from kats.consts import TimeSeriesData
from kats.transformers.t2v.consts import T2VParam
from kats.transformers.t2v.t2vbatch import T2VBatch, T2VBatched
from kats.transformers.t2v.t2vnn import T2VNN
from kats.transformers.t2v.t2vpreprocessing import T2VPreprocessing, T2VPreprocessed
from kats.transformers.t2v.utils import Normalize, Standardize


def _make_test_data(values: List[List[float]]) -> List[List[Dict[str, Any]]]:
    result = []
    step = pd.Timedelta(1, unit="nanoseconds")
    for line in values:
        row = []
        t = pd.Timestamp("1970-01-01 00:00:00")
        for val in line:
            row.append({"time": t, "value": val})
            t += step
        result.append(row)
    return result


BATCH_DATA = _make_test_data(
    values=[
        [28, 46, 67, 75, 44, 1, 26, 94, 35, 35, 25, 42],
        [26, 68, 19, 10, 73, 37, 5, 71, 22, 46, 89, 45],
        [11, 89, 12, 61, 81, 88, 96, 59, 42, 75, 99, 67],
        [4, 36, 71, 91, 30, 8, 50, 28, 77, 39, 40, 85],
        [10, 22, 0, 45, 20, 89, 35, 53, 86, 56, 0, 62],
        [53, 54, 39, 14, 20, 46, 72, 52, 8, 73, 51, 56],
        [25, 40, 34, 62, 24, 89, 74, 37, 1, 6, 81, 90],
        [33, 90, 16, 42, 58, 50, 53, 23, 24, 70, 51, 69],
        [87, 32, 48, 28, 62, 21, 25, 27, 84, 48, 70, 80],
        [83, 48, 19, 85, 91, 62, 60, 48, 70, 0, 95, 12],
        [93, 86, 50, 55, 82, 61, 31, 29, 28, 48, 44, 92],
        [29, 15, 39, 18, 17, 0, 77, 46, 65, 91, 93, 37],
        [50, 62, 3, 0, 7, 28, 54, 2, 31, 9, 73, 82],
        [33, 96, 86, 54, 91, 31, 49, 6, 92, 7, 64, 56],
        [66, 87, 86, 58, 71, 53, 66, 50, 96, 91, 7, 33],
        [34, 95, 87, 77, 31, 45, 15, 67, 36, 53, 84, 13],
        [94, 54, 47, 81, 6, 73, 6, 32, 22, 84, 18, 18],
        [35, 28, 59, 81, 1, 0, 46, 68, 19, 10, 1, 66],
        [86, 11, 19, 4, 36, 37, 93, 8, 97, 52, 43, 98],
        [85, 23, 80, 73, 29, 58, 86, 95, 80, 80, 13, 8],
        [39, 65, 24, 72, 21, 3, 25, 57, 28, 97, 86, 36],
        [74, 69, 17, 84, 41, 99, 40, 37, 33, 16, 36, 24],
        [75, 86, 85, 84, 26, 56, 30, 5, 90, 39, 97, 11],
        [52, 70, 9, 44, 16, 25, 84, 91, 61, 45, 63, 1],
        [53, 64, 50, 52, 35, 25, 28, 20, 10, 39, 10, 35],
        [58, 38, 98, 53, 97, 54, 23, 21, 68, 55, 32, 35],
        [19, 82, 98, 80, 61, 97, 72, 25, 65, 72, 72, 39],
        [16, 0, 88, 60, 42, 41, 24, 38, 34, 2, 43, 50],
        [93, 97, 11, 18, 43, 58, 48, 60, 16, 73, 56, 54],
        [46, 11, 61, 79, 87, 82, 7, 94, 20, 80, 86, 79],
        [69, 71, 24, 81, 88, 11, 14, 58, 25, 25, 46, 31],
        [9, 15, 70, 16, 22, 25, 84, 85, 6, 13, 78, 6],
        [8, 47, 71, 58, 86, 92, 81, 94, 93, 38, 98, 17],
        [58, 16, 13, 30, 23, 98, 59, 44, 97, 2, 36, 42],
        [39, 89, 54, 86, 38, 14, 3, 92, 85, 24, 12, 81],
        [32, 15, 41, 65, 54, 98, 41, 33, 29, 12, 12, 17],
        [31, 95, 98, 38, 45, 28, 61, 92, 61, 56, 15, 55],
        [9, 29, 24, 83, 4, 64, 93, 48, 2, 44, 13, 29],
        [67, 17, 61, 36, 24, 47, 64, 52, 78, 72, 14, 48],
        [87, 67, 11, 58, 36, 60, 42, 69, 38, 55, 62, 45],
        [87, 10, 61, 76, 97, 24, 70, 51, 3, 58, 71, 19],
        [92, 62, 53, 73, 97, 56, 89, 40, 2, 5, 4, 4],
        [53, 46, 86, 48, 8, 98, 19, 60, 34, 49, 81, 61],
        [16, 87, 2, 31, 98, 12, 88, 72, 67, 13, 97, 10],
        [55, 66, 91, 28, 8, 66, 24, 97, 75, 41, 8, 78],
        [39, 24, 4, 10, 58, 97, 87, 78, 37, 71, 4, 47],
        [89, 17, 69, 36, 59, 47, 76, 27, 68, 76, 80, 39],
        [44, 95, 61, 92, 57, 66, 55, 39, 97, 39, 90, 76],
        [32, 5, 87, 18, 79, 39, 91, 96, 58, 58, 47, 40],
        [12, 44, 88, 52, 41, 99, 62, 26, 37, 59, 6, 4],
        [44, 57, 30, 22, 41, 9, 31, 52, 29, 38, 30, 81],
        [99, 1, 77, 76, 92, 11, 38, 65, 95, 12, 45, 26],
        [16, 94, 97, 59, 63, 87, 34, 69, 82, 29, 70, 73],
        [95, 97, 62, 27, 59, 78, 93, 36, 92, 21, 67, 19],
        [95, 99, 67, 90, 63, 16, 11, 21, 16, 9, 64, 21],
        [89, 81, 49, 13, 63, 46, 28, 75, 35, 98, 51, 75],
        [6, 28, 10, 32, 5, 78, 31, 50, 74, 57, 86, 25],
        [97, 45, 97, 28, 3, 80, 98, 12, 38, 14, 28, 28],
        [74, 31, 1, 46, 77, 83, 21, 74, 26, 66, 80, 55],
        [91, 92, 20, 46, 79, 58, 83, 5, 18, 56, 68, 2],
        [0, 57, 37, 14, 33, 60, 34, 63, 24, 23, 11, 17],
        [14, 79, 26, 71, 59, 45, 88, 16, 92, 52, 7, 78],
        [26, 45, 49, 70, 2, 92, 63, 43, 52, 23, 59, 2],
        [19, 45, 14, 24, 93, 77, 70, 16, 83, 99, 8, 37],
        [47, 59, 62, 85, 1, 87, 71, 10, 63, 76, 80, 71],
        [20, 17, 93, 63, 81, 94, 37, 31, 10, 44, 88, 32],
        [40, 7, 10, 85, 50, 87, 40, 16, 75, 45, 31, 78],
        [79, 53, 85, 91, 19, 32, 73, 39, 31, 96, 1, 96],
        [32, 76, 29, 32, 76, 25, 21, 65, 37, 89, 45, 14],
        [97, 43, 48, 74, 60, 66, 5, 8, 5, 72, 94, 31],
        [40, 75, 7, 71, 49, 91, 61, 83, 61, 6, 74, 83],
        [79, 3, 5, 57, 21, 25, 2, 40, 59, 13, 74, 81],
        [11, 86, 11, 12, 24, 44, 18, 54, 99, 44, 7, 92],
        [52, 54, 31, 50, 43, 81, 69, 17, 82, 21, 36, 95],
        [55, 58, 2, 27, 73, 34, 60, 92, 53, 62, 78, 96],
        [6, 75, 48, 93, 84, 30, 57, 60, 23, 19, 36, 3],
        [91, 19, 59, 23, 49, 48, 50, 95, 35, 97, 99, 65],
        [78, 33, 35, 5, 38, 41, 57, 11, 2, 17, 39, 20],
        [14, 96, 98, 25, 7, 99, 93, 86, 41, 43, 13, 77],
        [1, 36, 68, 37, 40, 61, 59, 13, 88, 83, 1, 69],
        [16, 88, 27, 65, 34, 4, 64, 39, 75, 55, 35, 44],
        [22, 46, 30, 10, 9, 66, 13, 79, 73, 92, 9, 75],
        [93, 0, 4, 19, 7, 42, 75, 0, 56, 63, 83, 27],
        [36, 46, 75, 36, 48, 43, 55, 27, 78, 5, 60, 19],
        [77, 1, 72, 36, 19, 25, 28, 0, 34, 69, 58, 68],
        [32, 50, 77, 11, 19, 52, 87, 42, 97, 88, 49, 28],
        [56, 38, 94, 55, 42, 49, 22, 43, 37, 83, 22, 44],
        [51, 79, 49, 7, 84, 0, 7, 28, 95, 4, 85, 46],
        [33, 64, 54, 89, 42, 73, 29, 81, 16, 25, 7, 46],
        [75, 90, 71, 88, 45, 67, 32, 0, 18, 30, 79, 13],
        [52, 47, 68, 89, 61, 91, 65, 63, 69, 62, 72, 32],
        [40, 88, 47, 12, 72, 76, 30, 74, 67, 97, 3, 18],
        [69, 97, 81, 12, 22, 82, 74, 0, 87, 53, 70, 88],
        [84, 14, 64, 75, 70, 14, 20, 18, 59, 92, 88, 32],
        [48, 95, 34, 43, 89, 10, 98, 71, 77, 73, 65, 45],
        [8, 2, 84, 25, 71, 47, 65, 69, 27, 92, 25, 24],
        [11, 4, 97, 47, 36, 78, 43, 99, 14, 42, 47, 86],
        [69, 45, 42, 20, 22, 79, 46, 14, 24, 35, 74, 50],
        [14, 67, 20, 12, 85, 18, 33, 77, 67, 51, 26, 28],
        [27, 34, 7, 15, 35, 29, 17, 1, 48, 76, 3, 89],
    ]
)

# fmt: off
NN_DATA_TRAIN = _make_test_data(values=[
    [8., 10., 6., 9., 6., 6., 7., 0., 9., 9., 7., 3., 3., 3., 5.,
     6., 5., 0., 7., 9., 10., 6., 5., 5., 4., 5., 8., 9., 8., 9.],
    [7., 9., 8., 9., 7., 7., 0., 9., 10., 4., 3., 1., 7., 2., 3.,
     6., 9., 8., 1., 2., 3., 9., 0., 6., 1., 4., 0., 2., 1., 0.],
    [6., 4., 0., 5., 4., 2., 6., 10., 9., 8., 8., 8., 1., 2., 6.,
     4., 1., 2., 9., 5., 6., 8., 0., 10., 6., 7., 2., 3., 9., 5.],
    [9., 6., 3., 7., 4., 0., 3., 2., 8., 7., 0., 10., 5., 8., 6.,
     3., 0., 1., 4., 10., 4., 7., 5., 7., 1., 2., 5., 9., 2., 5.],
    [5., 1., 6., 1., 5., 0., 2., 1., 6., 3., 7., 5., 9., 3., 4.,
     8., 6., 9., 8., 6., 0., 7., 3., 2., 6., 2., 7., 5., 4., 3.],
    [7., 7., 0., 8., 4., 3., 5., 5., 1., 7., 6., 0., 4., 5., 3.,
     1., 3., 0., 6., 1., 1., 8., 4., 1., 10., 8., 8., 0., 10., 2.],
    [3., 6., 9., 3., 1., 6., 5., 2., 5., 10., 5., 4., 3., 9., 0.,
     9., 5., 1., 2., 7., 9., 5., 2., 6., 3., 8., 4., 2., 1., 10.],
    [1., 2., 1., 3., 1., 9., 8., 1., 8., 5., 0., 3., 0., 5., 3.,
     2., 2., 8., 5., 8., 4., 1., 7., 4., 1., 3., 8., 3., 3., 1.],
    [5., 10., 1., 6., 2., 2., 4., 2., 1., 9., 8., 1., 10., 1., 7.,
     8., 6., 2., 8., 2., 4., 8., 2., 6., 0., 3., 3., 6., 7., 0.],
    [4., 6., 3., 3., 3., 5., 6., 1., 4., 3., 2., 6., 10., 5., 6.,
     4., 5., 4., 6., 1., 10., 4., 5., 5., 4., 9., 6., 9., 7., 6.],
    [4., 0., 4., 9., 1., 4., 4., 1., 4., 3., 0., 2., 0., 10., 4.,
     3., 6., 4., 9., 3., 1., 3., 1., 7., 2., 8., 3., 5., 9., 4.],
    [9., 0., 8., 0., 7., 8., 8., 0., 1., 0., 9., 1., 9., 7., 4.,
     10., 6., 1., 7., 10., 5., 7., 0., 0., 6., 7., 2., 4., 0., 6.],
    [4., 7., 7., 8., 6., 10., 0., 2., 7., 2., 7., 0., 4., 3., 4.,
     7., 9., 3., 10., 3., 2., 6., 2., 2., 10., 9., 4., 4., 5., 7.],
    [9., 5., 0., 7., 7., 8., 8., 1., 1., 2., 0., 3., 3., 8., 8.,
     8., 9., 6., 1., 9., 4., 3., 10., 7., 1., 5., 4., 1., 10., 9.],
    [0., 7., 1., 6., 8., 8., 7., 10., 9., 8., 1., 5., 3., 3., 4.,
     2., 0., 7., 4., 2., 9., 6., 6., 0., 8., 1., 4., 4., 7., 7.],
    [6., 6., 10., 5., 5., 10., 8., 5., 1., 1., 4., 3., 10., 1., 2.,
     6., 7., 5., 10., 4., 0., 7., 2., 5., 2., 6., 6., 8., 4., 7.],
    [9., 9., 7., 0., 4., 2., 0., 3., 7., 2., 9., 1., 10., 8., 9.,
     4., 6., 10., 10., 1., 8., 8., 3., 3., 5., 9., 9., 9., 5., 0.],
    [8., 1., 4., 8., 3., 10., 2., 6., 5., 7., 2., 4., 9., 3., 3.,
     3., 9., 7., 5., 2., 9., 4., 5., 4., 5., 7., 6., 4., 9., 5.],
    [3., 8., 3., 0., 6., 6., 3., 2., 7., 4., 9., 6., 3., 3., 7.,
     7., 1., 1., 2., 2., 0., 2., 5., 7., 9., 5., 4., 3., 2., 3.],
    [6., 0., 6., 0., 1., 3., 0., 10., 10., 10., 8., 5., 0., 0., 5.,
     3., 3., 10., 5., 10., 6., 8., 9., 4., 8., 3., 9., 1., 2., 10.],
    [0., 5., 5., 0., 10., 2., 4., 2., 9., 2., 9., 2., 4., 4., 3.,
     0., 4., 9., 4., 3., 5., 9., 3., 6., 6., 1., 2., 0., 7., 8.],
    [3., 5., 8., 5., 8., 3., 7., 8., 3., 4., 7., 9., 5., 6., 2.,
     6., 7., 9., 5., 9., 8., 5., 8., 3., 7., 10., 3., 1., 9., 10.],
    [8., 4., 3., 1., 0., 10., 6., 5., 1., 7., 10., 3., 7., 6., 4.,
     9., 9., 7., 8., 4., 9., 4., 1., 4., 2., 6., 9., 4., 7., 0.],
    [2., 2., 8., 4., 8., 8., 0., 1., 4., 3., 9., 8., 6., 5., 6.,
     0., 0., 7., 1., 9., 10., 8., 6., 5., 0., 0., 1., 5., 5., 1.],
    [0., 9., 4., 4., 1., 5., 8., 1., 9., 1., 2., 6., 5., 7., 7.,
     5., 10., 9., 2., 2., 0., 6., 4., 3., 9., 3., 3., 5., 7., 7.],
    [4., 5., 3., 10., 8., 7., 9., 9., 0., 1., 8., 7., 2., 7., 0.,
     9., 3., 2., 5., 6., 3., 6., 8., 10., 1., 10., 2., 4., 2., 10.],
    [7., 9., 7., 7., 8., 3., 4., 4., 7., 6., 8., 7., 1., 9., 3.,
     5., 1., 6., 2., 2., 10., 1., 10., 7., 5., 0., 0., 6., 7., 4.],
    [10., 4., 3., 5., 5., 2., 8., 10., 6., 0., 6., 4., 4., 1., 3.,
     5., 9., 10., 6., 10., 4., 8., 2., 0., 3., 8., 3., 1., 9., 4.],
    [2., 8., 3., 5., 1., 5., 2., 8., 10., 1., 8., 4., 7., 1., 5.,
     2., 9., 4., 7., 9., 0., 4., 2., 3., 1., 8., 2., 1., 9., 10.],
    [8., 6., 5., 0., 4., 10., 10., 2., 0., 3., 1., 1., 2., 7., 7.,
     6., 5., 8., 3., 0., 7., 6., 6., 2., 3., 9., 8., 5., 4., 2.],
    [8., 4., 4., 3., 3., 0., 6., 8., 2., 1., 2., 0., 5., 10., 10.,
     6., 10., 10., 1., 7., 5., 3., 4., 7., 2., 2., 10., 3., 0., 8.],
    [7., 7., 9., 5., 6., 5., 8., 2., 2., 2., 9., 2., 8., 3., 6.,
     8., 10., 6., 8., 2., 3., 7., 5., 4., 3., 7., 0., 8., 0., 7.],
    [3., 8., 10., 4., 2., 8., 10., 8., 3., 10., 3., 7., 1., 8., 5.,
     8., 7., 6., 0., 2., 10., 1., 4., 4., 4., 3., 4., 2., 0., 4.],
    [8., 8., 3., 2., 0., 1., 7., 0., 7., 6., 7., 5., 8., 2., 9.,
     3., 5., 7., 4., 2., 5., 3., 8., 2., 3., 6., 6., 3., 5., 2.],
    [5., 1., 7., 1., 10., 6., 8., 0., 7., 6., 5., 5., 5., 8., 1.,
     0., 5., 9., 10., 7., 2., 9., 0., 8., 0., 2., 7., 2., 3., 7.],
    [0., 5., 0., 0., 7., 10., 3., 0., 9., 6., 4., 4., 0., 9., 10.,
     5., 8., 8., 7., 4., 0., 8., 0., 7., 4., 2., 10., 6., 6., 8.],
    [6., 10., 2., 10., 0., 7., 2., 7., 7., 4., 0., 9., 6., 9., 4.,
     7., 8., 10., 6., 6., 0., 3., 1., 1., 1., 3., 0., 3., 1., 4.],
    [2., 7., 3., 0., 0., 9., 2., 8., 6., 1., 8., 9., 1., 2., 10.,
     9., 1., 2., 5., 8., 8., 6., 2., 3., 5., 10., 4., 1., 5., 2.],
    [7., 1., 0., 8., 2., 10., 0., 6., 9., 4., 4., 0., 8., 6., 7.,
     8., 5., 4., 5., 10., 1., 3., 2., 6., 8., 4., 5., 9., 1., 2.],
    [2., 8., 6., 7., 10., 9., 10., 8., 5., 3., 0., 7., 6., 1., 7.,
     4., 6., 3., 4., 7., 0., 0., 3., 2., 9., 1., 2., 10., 10., 10.],
    [0., 0., 5., 4., 7., 1., 3., 7., 5., 0., 3., 0., 1., 4., 7.,
     7., 10., 2., 10., 3., 9., 10., 5., 10., 10., 9., 2., 1., 5., 10.],
    [4., 9., 4., 9., 4., 9., 4., 2., 6., 5., 3., 1., 8., 0., 10.,
     3., 0., 5., 3., 4., 4., 8., 0., 1., 5., 4., 10., 6., 2., 2.],
    [9., 1., 2., 9., 0., 4., 4., 10., 6., 3., 2., 0., 7., 2., 1.,
     10., 3., 2., 0., 2., 1., 7., 5., 8., 7., 9., 2., 4., 6., 4.],
    [7., 4., 0., 4., 3., 5., 2., 10., 4., 1., 10., 5., 5., 4., 5.,
     10., 6., 4., 0., 7., 3., 8., 9., 5., 0., 2., 2., 9., 3., 5.],
    [6., 9., 3., 5., 10., 0., 7., 6., 10., 10., 8., 2., 1., 0., 2.,
     4., 10., 1., 2., 3., 6., 2., 8., 7., 3., 0., 0., 0., 0., 7.],
    [2., 5., 2., 10., 4., 6., 1., 5., 5., 10., 2., 5., 9., 9., 4.,
     1., 8., 7., 2., 7., 8., 2., 2., 1., 7., 9., 0., 7., 6., 6.],
    [2., 0., 3., 8., 4., 0., 4., 7., 7., 4., 8., 0., 3., 2., 1.,
     7., 6., 8., 1., 3., 5., 10., 2., 7., 2., 1., 2., 10., 5., 1.],
    [7., 0., 6., 1., 9., 5., 1., 1., 9., 7., 1., 6., 2., 7., 6.,
     7., 6., 10., 5., 6., 4., 6., 0., 0., 8., 4., 4., 2., 4., 2.],
    [8., 8., 5., 0., 6., 6., 9., 9., 3., 7., 5., 3., 3., 1., 1.,
     3., 4., 7., 5., 3., 10., 6., 7., 1., 6., 1., 1., 7., 1., 10.],
    [8., 0., 8., 0., 9., 9., 1., 5., 5., 0., 6., 7., 1., 7., 8.,
     5., 4., 8., 10., 8., 0., 10., 2., 8., 10., 1., 3., 3., 1., 2.],
    [2., 0., 5., 7., 2., 4., 0., 9., 6., 9., 0., 10., 8., 7., 6.,
     9., 6., 4., 4., 10., 4., 0., 7., 10., 0., 3., 8., 8., 4., 3.],
    [2., 2., 4., 9., 0., 4., 5., 6., 8., 2., 1., 2., 8., 6., 8.,
     1., 4., 3., 1., 8., 8., 5., 10., 5., 7., 5., 0., 3., 6., 1.],
    [8., 7., 7., 6., 3., 1., 2., 5., 4., 7., 5., 5., 0., 3., 4.,
     10., 2., 8., 0., 5., 5., 9., 4., 4., 9., 4., 5., 7., 1., 5.],
    [0., 3., 9., 2., 8., 2., 5., 0., 5., 3., 7., 10., 8., 3., 9.,
     9., 5., 0., 3., 6., 0., 7., 9., 10., 5., 4., 9., 10., 8., 1.],
    [4., 5., 6., 4., 8., 0., 6., 9., 6., 3., 8., 10., 4., 3., 6.,
     3., 2., 8., 10., 7., 0., 6., 8., 7., 7., 5., 10., 3., 7., 2.],
    [8., 1., 1., 6., 9., 3., 9., 7., 8., 6., 4., 7., 3., 1., 10.,
     5., 8., 0., 10., 9., 6., 9., 8., 9., 8., 9., 10., 2., 10., 4.],
    [7., 7., 3., 10., 10., 0., 0., 3., 5., 7., 5., 8., 8., 1., 9.,
     0., 1., 4., 3., 3., 8., 10., 7., 5., 0., 1., 5., 10., 1., 9.],
    [4., 4., 5., 5., 5., 1., 9., 2., 5., 7., 10., 0., 0., 10., 1.,
     10., 5., 8., 3., 2., 7., 10., 9., 8., 10., 4., 8., 1., 2., 6.],
    [6., 5., 5., 1., 9., 5., 3., 2., 10., 1., 0., 10., 10., 4., 1.,
     1., 6., 7., 9., 7., 3., 0., 10., 6., 0., 10., 3., 7., 1., 5.],
    [5., 8., 4., 9., 3., 4., 9., 10., 4., 7., 9., 7., 1., 6., 7.,
     10., 5., 4., 10., 8., 2., 9., 3., 8., 5., 7., 6., 10., 1., 2.],
    [0., 8., 6., 0., 2., 3., 3., 4., 2., 2., 0., 6., 10., 9., 0.,
     1., 3., 3., 4., 10., 1., 3., 0., 1., 0., 9., 9., 3., 3., 6.],
    [9., 10., 10., 8., 8., 4., 9., 0., 0., 5., 1., 7., 8., 3., 4.,
     10., 7., 2., 4., 2., 9., 2., 2., 6., 6., 6., 9., 7., 4., 4.],
    [3., 6., 9., 3., 1., 3., 7., 8., 2., 6., 8., 10., 3., 9., 2.,
     4., 0., 2., 7., 10., 7., 3., 1., 7., 7., 9., 3., 2., 1., 8.],
    [0., 6., 4., 7., 3., 6., 4., 4., 8., 8., 8., 5., 5., 4., 8.,
     9., 7., 6., 6., 5., 4., 3., 2., 2., 5., 3., 5., 2., 3., 8.]
])

NN_DATA_TEST = _make_test_data(values=[
    [4., 6., 9., 1., 4., 8., 10., 2., 8., 10., 5., 3., 6., 7., 5., 4., 5., 4.,
     7., 0., 10., 6., 9., 9., 10., 1., 8., 0., 10., 9., ],
    [5., 8., 0., 3., 3., 8., 10., 4., 4., 5., 0., 0., 8., 2., 3., 0., 8., 7.,
     1., 7., 6., 10., 5., 0., 6., 9., 6., 8., 0., 1., ],
    [10., 8., 8., 5., 5., 3., 3., 4., 2., 0., 0., 6., 0., 0., 2., 9., 3., 3.,
     1., 0., 2., 10., 3., 2., 7., 9., 0., 1., 6., 3., ],
    [7., 10., 2., 5., 8., 0., 0., 7., 2., 4., 0., 4., 10., 5., 7., 2., 2., 10.,
     10., 9., 8., 3., 3., 6., 1., 7., 5., 2., 8., 4., ],
    [7., 6., 7., 6., 2., 1., 6., 2., 2., 1., 3., 7., 0., 0., 4., 10., 4., 10.,
     0., 5., 0., 4., 8., 4., 5., 7., 10., 0., 1., 8., ],
    [2., 5., 4., 10., 1., 6., 3., 0., 3., 5., 6., 6., 6., 2., 5., 7., 3., 7.,
     1., 9., 8., 8., 7., 9., 9., 0., 1., 5., 9., 4., ],
    [8., 4., 1., 9., 3., 8., 0., 10., 0., 6., 10., 2., 9., 1., 8., 2., 1., 7.,
     9., 5., 5., 9., 10., 6., 6., 10., 5., 3., 10., 8., ],
    [5., 3., 2., 3., 2., 1., 4., 1., 5., 2., 5., 10., 9., 8., 0., 5., 8., 10.,
     3., 10., 1., 3., 0., 2., 9., 8., 9., 4., 10., 5., ],
    [2., 4., 10., 7., 5., 3., 1., 8., 0., 0., 3., 6., 0., 8., 3., 2., 0., 9.,
     3., 8., 9., 5., 5., 0., 9., 1., 10., 5., 1., 2., ],
    [3., 10., 4., 8., 3., 10., 8., 10., 9., 2., 7., 0., 10., 8., 8., 9., 1.,
     3., 7., 6., 0., 4., 6., 3., 0., 8., 4., 10., 1., 7., ],
    [10., 1., 2., 9., 0., 10., 1., 6., 9., 1., 5., 2., 3., 10., 7., 2., 9., 5.,
     0., 6., 7., 10., 7., 2., 9., 0., 8., 5., 10., 8., ],
    [6., 0., 0., 5., 4., 8., 1., 7., 10., 4., 1., 9., 10., 10., 5., 9., 7., 3.,
     1., 10., 4., 1., 1., 5., 10., 9., 8., 3., 5., 6., ],
    [10., 5., 7., 3., 0., 6., 2., 5., 1., 8., 0., 5., 5., 4., 4., 0., 9., 4.,
     1., 6., 2., 3., 2., 2., 3., 4., 0., 3., 10., 7., ],
    [9., 6., 8., 4., 9., 3., 7., 8., 6., 6., 4., 3., 9., 0., 9., 9., 6., 0.,
     0., 4., 8., 5., 2., 2., 6., 5., 8., 5., 4., 9., ],
    [6., 0., 6., 2., 8., 5., 8., 4., 6., 3., 6., 0., 6., 7., 4., 1., 4., 5.,
     1., 5., 5., 0., 0., 10., 9., 2., 9., 9., 2., 3., ],
    [1., 8., 9., 10., 6., 6., 5., 2., 5., 7., 7., 10., 8., 0., 0., 2., 2., 9.,
     7., 9., 0., 1., 1., 2., 5., 0., 1., 0., 4., 6., ],
    [7., 9., 0., 6., 4., 9., 0., 2., 6., 9., 0., 6., 4., 0., 0., 0., 10., 8.,
     10., 2., 4., 10., 9., 8., 5., 8., 7., 7., 0., 8., ],
    [5., 8., 2., 2., 5., 8., 6., 2., 2., 1., 8., 2., 10., 4., 8., 7., 1., 2.,
     1., 7., 5., 5., 8., 7., 3., 3., 8., 0., 1., 1., ],
    [5., 3., 10., 9., 2., 2., 2., 5., 7., 10., 6., 3., 10., 3., 10., 9., 10.,
     10., 10., 1., 0., 8., 1., 7., 8., 3., 7., 8., 9., 9., ],
    [4., 6., 9., 3., 0., 2., 4., 2., 10., 6., 8., 5., 9., 0., 6., 2., 4., 9.,
     3., 9., 4., 4., 5., 9., 9., 10., 5., 7., 7., 10., ],
    [8., 7., 10., 1., 9., 10., 0., 9., 1., 1., 2., 9., 6., 1., 0., 5., 8., 2.,
     5., 2., 0., 4., 9., 10., 3., 6., 5., 9., 6., 4., ],
    [3., 6., 6., 5., 7., 3., 9., 4., 3., 0., 7., 0., 4., 5., 5., 4., 2., 4.,
     7., 1., 5., 8., 7., 10., 7., 7., 6., 7., 5., 1., ],
    [1., 7., 10., 0., 5., 5., 10., 3., 10., 10., 9., 2., 10., 2., 0., 2., 3.,
     10., 10., 4., 2., 5., 10., 4., 2., 5., 5., 7., 1., 0., ],
    [3., 6., 10., 9., 2., 2., 4., 4., 9., 2., 8., 3., 2., 10., 1., 9., 10., 8.,
     3., 2., 6., 7., 10., 1., 6., 4., 10., 4., 2., 4., ],
    [1., 3., 5., 0., 9., 3., 5., 8., 10., 10., 1., 10., 6., 5., 1., 7., 6.,
     10., 4., 4., 6., 8., 10., 8., 8., 5., 9., 5., 2., 0., ],
    [6., 3., 0., 7., 5., 5., 1., 0., 1., 3., 1., 0., 6., 10., 10., 7., 5., 3.,
     5., 2., 7., 9., 1., 4., 0., 5., 1., 3., 5., 9., ],
    [10., 7., 7., 10., 5., 3., 7., 7., 9., 0., 3., 3., 6., 10., 6., 10., 7.,
     10., 4., 6., 7., 8., 2., 2., 3., 4., 9., 0., 6., 8., ],
    [7., 4., 3., 6., 4., 6., 0., 0., 9., 2., 10., 1., 1., 6., 9., 1., 10., 3.,
     1., 5., 7., 9., 3., 4., 3., 2., 4., 8., 7., 7., ],
    [7., 7., 6., 7., 10., 7., 7., 9., 6., 0., 3., 3., 4., 9., 5., 3., 1., 5.,
     7., 7., 6., 5., 1., 6., 10., 4., 5., 9., 7., 7., ],
    [2., 6., 0., 0., 6., 9., 8., 6., 9., 4., 7., 1., 8., 5., 8., 1., 2., 7.,
     0., 9., 6., 6., 0., 8., 7., 3., 9., 10., 4., 1., ],
    [8., 5., 2., 6., 8., 2., 1., 6., 6., 3., 10., 8., 7., 0., 0., 4., 5., 2.,
     8., 9., 5., 3., 6., 7., 8., 10., 5., 5., 7., 4., ],
    [2., 2., 5., 1., 5., 5., 7., 6., 7., 8., 5., 2., 9., 2., 10., 0., 10., 4.,
     3., 8., 8., 4., 9., 3., 6., 5., 3., 9., 1., 0., ],
    [3., 3., 0., 5., 3., 0., 0., 0., 7., 9., 7., 1., 10., 6., 7., 7., 10., 9.,
     10., 6., 4., 0., 8., 6., 1., 5., 1., 0., 6., 1., ],
    [1., 10., 4., 4., 6., 0., 7., 2., 3., 8., 3., 9., 4., 6., 3., 0., 8., 3.,
     8., 5., 0., 2., 10., 1., 8., 0., 8., 8., 7., 7., ],
    [10., 0., 10., 7., 9., 2., 10., 6., 10., 0., 0., 9., 4., 4., 7., 6., 7.,
     5., 5., 1., 5., 4., 2., 5., 7., 8., 0., 2., 2., 10., ],
    [7., 0., 4., 2., 1., 7., 2., 6., 10., 0., 5., 2., 7., 5., 1., 5., 6., 4.,
     2., 3., 3., 5., 9., 0., 10., 3., 9., 3., 9., 0., ],
    [8., 3., 6., 2., 9., 2., 7., 7., 10., 0., 9., 1., 7., 0., 7., 7., 1., 3.,
     5., 10., 8., 7., 7., 6., 8., 1., 2., 9., 7., 5., ],
    [9., 8., 9., 3., 7., 9., 2., 2., 7., 4., 2., 9., 10., 0., 2., 8., 4., 9.,
     1., 6., 1., 3., 6., 3., 6., 6., 0., 5., 8., 1., ],
    [2., 9., 4., 8., 3., 0., 3., 5., 4., 0., 3., 5., 9., 9., 3., 8., 4., 7.,
     8., 9., 4., 5., 0., 10., 6., 3., 10., 1., 4., 2., ],
    [5., 7., 6., 2., 7., 1., 9., 8., 3., 0., 7., 6., 10., 4., 9., 4., 6., 4.,
     7., 10., 5., 4., 8., 10., 1., 0., 7., 2., 10., 6., ],
    [9., 10., 7., 5., 9., 8., 6., 9., 4., 7., 1., 6., 3., 8., 0., 6., 5., 7.,
     4., 0., 0., 8., 9., 5., 9., 4., 0., 5., 5., 2., ],
    [7., 0., 0., 5., 9., 0., 5., 0., 5., 2., 7., 5., 10., 0., 4., 7., 2., 5.,
     4., 5., 9., 0., 6., 8., 1., 9., 4., 9., 3., 5., ],
    [5., 10., 1., 1., 3., 0., 8., 6., 6., 2., 10., 2., 0., 2., 2., 3., 3., 2.,
     9., 7., 7., 0., 6., 10., 1., 0., 2., 3., 5., 9., ],
    [4., 3., 7., 10., 3., 9., 1., 2., 5., 7., 7., 10., 8., 6., 1., 7., 6., 7.,
     2., 6., 9., 8., 2., 9., 5., 1., 1., 4., 4., 6., ],
    [1., 10., 6., 4., 5., 7., 9., 3., 6., 5., 9., 1., 7., 9., 8., 4., 9., 3.,
     5., 7., 10., 2., 9., 10., 10., 0., 3., 3., 0., 8., ],
    [2., 4., 5., 7., 5., 2., 5., 0., 3., 5., 6., 10., 10., 9., 2., 8., 6., 0.,
     1., 0., 10., 8., 8., 6., 9., 2., 4., 3., 6., 8., ],
    [1., 7., 8., 9., 9., 8., 9., 6., 8., 8., 2., 3., 1., 9., 2., 10., 10., 4.,
     4., 5., 6., 6., 0., 1., 8., 9., 1., 5., 10., 10., ],
    [0., 2., 1., 2., 4., 10., 9., 5., 8., 10., 9., 3., 6., 5., 1., 9., 10.,
     10., 10., 7., 6., 6., 7., 10., 0., 0., 7., 3., 4., 7., ],
    [4., 9., 3., 5., 4., 6., 9., 6., 3., 4., 2., 3., 3., 2., 9., 4., 4., 10.,
     6., 3., 3., 8., 5., 3., 1., 5., 1., 4., 5., 3., ],
    [2., 6., 1., 10., 5., 3., 5., 3., 3., 3., 8., 6., 2., 2., 5., 5., 3., 8.,
     3., 8., 3., 1., 1., 4., 3., 6., 10., 4., 5., 9., ],
    [3., 1., 9., 8., 9., 6., 3., 2., 9., 2., 8., 4., 2., 4., 1., 10., 3., 9.,
     1., 1., 7., 0., 6., 4., 10., 7., 0., 10., 6., 4., ],
    [3., 6., 9., 1., 0., 0., 10., 0., 4., 9., 6., 1., 9., 7., 3., 2., 9., 10.,
     7., 1., 10., 10., 2., 2., 9., 10., 0., 5., 1., 0., ],
    [1., 5., 3., 5., 8., 9., 9., 7., 0., 7., 8., 5., 9., 4., 5., 5., 8., 3.,
     1., 10., 10., 8., 1., 9., 8., 1., 7., 8., 9., 8., ],
    [9., 8., 1., 3., 6., 1., 0., 2., 3., 7., 5., 10., 1., 8., 1., 4., 3., 6.,
     7., 4., 9., 4., 1., 7., 1., 9., 5., 7., 5., 3., ],
    [3., 5., 3., 7., 8., 0., 3., 3., 1., 5., 2., 6., 10., 10., 4., 3., 4., 7.,
     2., 6., 5., 0., 6., 6., 0., 8., 5., 1., 4., 10., ],
    [3., 6., 7., 10., 10., 0., 4., 2., 0., 2., 5., 6., 7., 10., 3., 2., 4., 8.,
     5., 8., 5., 1., 4., 10., 8., 7., 6., 0., 3., 6., ],
    [10., 6., 5., 7., 10., 5., 9., 2., 3., 2., 0., 9., 4., 0., 0., 1., 3., 5.,
     7., 2., 0., 2., 2., 4., 10., 3., 3., 8., 6., 3., ],
    [2., 5., 2., 5., 10., 10., 8., 5., 3., 4., 5., 10., 2., 7., 3., 7., 9., 9.,
     2., 9., 6., 10., 8., 3., 7., 2., 6., 2., 6., 10., ],
    [9., 9., 10., 10., 5., 1., 10., 2., 5., 3., 5., 0., 9., 0., 2., 7., 7.,
     10., 3., 5., 7., 6., 10., 6., 3., 8., 9., 5., 10., 2., ],
    [4., 4., 4., 7., 8., 8., 5., 5., 8., 3., 5., 3., 10., 0., 6., 9., 10., 5.,
     8., 6., 6., 1., 10., 8., 9., 8., 5., 2., 0., 3., ],
    [2., 0., 2., 3., 7., 8., 2., 5., 9., 10., 4., 5., 3., 9., 5., 6., 8., 9.,
     6., 5., 0., 1., 0., 8., 9., 9., 8., 6., 1., 5., ],
    [10., 3., 0., 0., 4., 1., 3., 0., 3., 10., 5., 9., 3., 0., 6., 1., 8., 2.,
     7., 7., 2., 3., 1., 3., 10., 10., 10., 8., 1., 2., ],
    [10., 10., 3., 4., 0., 6., 9., 6., 8., 3., 5., 10., 1., 3., 1., 9., 4., 6.,
     0., 9., 9., 6., 9., 5., 5., 7., 7., 5., 2., 4., ],
    [1., 4., 8., 9., 0., 9., 4., 5., 10., 6., 8., 5., 4., 6., 0., 10., 4., 1.,
     3., 6., 4., 5., 1., 2., 6., 10., 1., 5., 4., 9., ],
    ]
)

NN_LABELS = np.asarray([
    1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
    0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
    0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
])

NN_LABELS = np.asarray([
    1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
    0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
    0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
])
# fmt: on


class test_preprocessing(TestCase):
    @classmethod
    def setUpClass(cls) -> None:
        # Create dummy params
        ts = pd.DataFrame([np.arange(12), np.arange(12)]).transpose()
        ts.columns = ["time", "value"]
        cls.ts = ts = [TimeSeriesData(ts)]
        cls.reg_param = T2VParam(mode="regression", normalizer=Normalize)
        cls.class_param = T2VParam(mode="classification", normalizer=Normalize)

    def run_preprocessing(
        self,
        ts: Optional[List[TimeSeriesData]] = None,
        param: Optional[T2VParam] = None,
        label: Optional[List] = None,
    ) -> T2VPreprocessed:
        if ts is None:
            ts = self.ts
        if param is None:
            param = self.reg_param
        preprocessor = T2VPreprocessing(data=ts, param=param, label=label)
        preprocessed = preprocessor.transform()
        return preprocessed

    def test_normalize_regression(self) -> None:
        preprocessed = self.run_preprocessing()
        self.assertEqual(preprocessed.label, [11])
        self.assertTrue(
            np.array_equal(preprocessed.seq[0], ((np.arange(11)) / 10).reshape([11, 1]))
        )
        self.assertEqual(preprocessed.output_size, 1)
        self.assertEqual(preprocessed.window, 11)

    def test_standardize_regression(self) -> None:
        preprocessed = self.run_preprocessing(
            param=T2VParam(mode="regression", normalizer=Standardize)
        )
        self.assertTrue(
            np.array_equal(
                preprocessed.seq[0],
                (
                    (np.arange(11) - np.mean(np.arange(11))) / np.std(np.arange(11))
                ).reshape([11, 1]),
            )
        )
        self.assertFalse(preprocessed.batched)

    def test_user_label_regression(self) -> None:
        preprocessed = self.run_preprocessing(label=[1.25])
        self.assertEqual(preprocessed.label, [1.25])
        self.assertTrue(
            np.array_equal(preprocessed.seq[0], ((np.arange(12)) / 11).reshape([12, 1]))
        )
        self.assertTrue(preprocessed.output_size == 1)
        self.assertTrue(preprocessed.window == 12)

    def test_normalize_classification(self) -> None:
        preprocessed = self.run_preprocessing(param=self.class_param, label=[2])
        self.assertEqual(preprocessed.label, [2])
        self.assertTrue(
            np.array_equal(preprocessed.seq[0], ((np.arange(12)) / 11).reshape([12, 1]))
        )
        self.assertEqual(preprocessed.output_size, 3)

    def test_standardize_classification(self) -> None:
        preprocessed = self.run_preprocessing(
            param=T2VParam(mode="classification", normalizer=Standardize),
            label=[0],
        )
        self.assertEqual(preprocessed.label, [0])
        self.assertTrue(
            np.array_equal(
                preprocessed.seq[0],
                (
                    (np.arange(12) - np.mean(np.arange(12))) / np.std(np.arange(12))
                ).reshape([12, 1]),
            )
        )
        self.assertEqual(preprocessed.output_size, 1)

    def test_invalid_label_regression(self) -> None:
        # Additional labels should throw an error
        self.assertRaises(ValueError, self.run_preprocessing, None, None, [1, 2])

    def test_invalid_label_classification(self) -> None:
        # Floats are an invalid output for classification
        self.assertRaises(
            ValueError, self.run_preprocessing, None, self.class_param, [1.55]
        )
        # Additional labels should throw an error
        self.assertRaises(
            ValueError, self.run_preprocessing, None, self.class_param, [1, 2]
        )

    def test_no_label_classification(self) -> None:
        # No labels should throw an error
        self.assertRaises(ValueError, self.run_preprocessing, None, self.class_param)


class test_batch(TestCase):
    @classmethod
    def setUpClass(cls) -> None:
        # Create dummy params
        cls.TS = []
        for ts in BATCH_DATA:
            cls.TS.append(TimeSeriesData(pd.DataFrame(ts)))
        cls.param = T2VParam(
            normalizer=Normalize,
            batch_size=32,
        )
        cls.batched, cls.preprocessed = cls.run_preprocessing()

    @classmethod
    def run_preprocessing(
        cls,
        ts: Optional[List[TimeSeriesData]] = None,
        param: Optional[T2VParam] = None,
        label: Optional[List] = None,
    ) -> Tuple[T2VBatched, T2VPreprocessed]:
        if ts is None:
            ts = cls.TS
        if param is None:
            param = cls.param
        preprocessor = T2VPreprocessing(data=ts, param=param, label=label)
        preprocessed = preprocessor.transform()
        batched = T2VBatch(preprocessed, param).transform()
        return batched, preprocessed

    def test_dims(self) -> None:
        self.assertEqual(len(self.batched.seq), 100)
        self.assertEqual(len(self.batched.batched_tensors[0]), 32)
        self.assertEqual(len(self.batched.batched_tensors[-1]), 4)
        self.assertEqual(self.batched.batch_size, 32)

    def test_attrs(self) -> None:
        self.assertTrue(np.array_equal(self.batched.label, self.preprocessed.label))
        self.assertTrue(np.array_equal(self.batched.seq, self.preprocessed.seq))
        self.assertEqual(self.batched.window, self.preprocessed.window)
        self.assertTrue(self.batched.batched)
        self.assertFalse(self.preprocessed.batched)

    def test_vals(self) -> None:
        self.assertTrue(
            np.array_equal(
                self.batched.batched_tensors[0][0][0].numpy(),
                self.preprocessed.seq[0].reshape([self.batched.window, 1]),
            )
        )
        self.assertTrue(
            np.array_equal(
                self.batched.batched_tensors[0][-1][0].numpy(),
                self.preprocessed.seq[31].reshape([self.batched.window, 1]),
            )
        )
        self.assertTrue(
            self.batched.batched_tensors[0][0][1].item() == self.preprocessed.label[0]
        )
        self.assertTrue(
            np.array_equal(
                self.batched.batched_tensors[-1][-1][0].numpy(),
                self.preprocessed.seq[-1].reshape([self.batched.window, 1]),
            )
        )
        self.assertEqual(
            self.batched.batched_tensors[-1][-1][1].item(), self.preprocessed.label[-1]
        )


class test_t2vnn(TestCase):
    @classmethod
    def setUpClass(cls) -> None:
        # Create dummy params
        cls.TS = []
        for ts in NN_DATA_TRAIN:
            cls.TS.append(TimeSeriesData(pd.DataFrame(ts)))

        cls.test_TS = []
        for ts in NN_DATA_TEST:
            cls.test_TS.append(TimeSeriesData(pd.DataFrame(ts)))

        cls.param_reg = T2VParam(
            mode="regression",
            normalizer=Normalize,
            batch_size=32,
            vector_length=16,
            learning_rate=0.001,
            hidden=[64],
            dropout=0.2,
            epochs=2,
        )
        cls.param_class = T2VParam(
            mode="classification",
            normalizer=Normalize,
            batch_size=32,
            vector_length=16,
            learning_rate=0.001,
            hidden=[64],
            dropout=0.2,
            epochs=2,
        )
        cls.label = NN_LABELS

    def run_preprocessing(
        self,
        dummy_label: bool = False,
        ts: Optional[List[TimeSeriesData]] = None,
        param: Optional[T2VParam] = None,
        label: Optional[List] = None,
    ) -> Tuple[T2VBatched, T2VPreprocessed]:
        if ts is None:
            ts = self.TS
        if param is None:
            param = self.param_reg
        if not dummy_label:
            preprocessor = T2VPreprocessing(data=ts, param=param, label=label)
        else:
            preprocessor = T2VPreprocessing(data=ts, param=param, dummy_label=True)
        preprocessed = preprocessor.transform()
        batched = T2VBatch(preprocessed, param).transform()
        return batched, preprocessed

    def test_regression(self) -> None:
        batched_train, preprocessed_train = self.run_preprocessing()
        _, preprocessed_test = self.run_preprocessing(ts=self.test_TS)
        t2vnn = T2VNN(batched_train, self.param_reg)
        t2vnn.train()

        # Test Output
        val_output = t2vnn.val(preprocessed_train)
        self.assertIsNotNone(val_output)
        self.assertEqual(list(val_output.keys())[0], "mae")

        # Test Embeddings
        test_embeddings = t2vnn.translate(preprocessed_test)
        self.assertEqual(len(test_embeddings[0]), 16)

    def test_classification(self) -> None:
        batched_train, preprocessed_train = self.run_preprocessing(
            param=self.param_class, label=self.label
        )
        batched_test, preprocessed_test = self.run_preprocessing(
            ts=self.test_TS, param=self.param_class, dummy_label=True
        )
        t2vnn = T2VNN(batched_train, self.param_class)
        train_translated = t2vnn.train(translate=True)

        # Test translation
        expected = batched_train.batched_tensors[0][0][1].item()
        self.assertIsNotNone(train_translated)
        self.assertEqual(expected, train_translated["labels"][0])

        # Test embeddings
        test_embeddings = t2vnn.translate(batched_test)
        self.assertEqual(len(test_embeddings[0]), 16)

        # Test on preprocessed data
        t2vnn = T2VNN(preprocessed_train, self.param_class)
        t2vnn.train()
        test_embeddings = t2vnn.translate(batched_test)
        self.assertEqual(len(test_embeddings[0]), 16)
